{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "693a1167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchshard as ts\n",
    "from torch import nn, optim\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn import functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "770139ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15483, 24576])\n"
     ]
    }
   ],
   "source": [
    "X = torch.load('X.pt')\n",
    "X = X.view(X.shape[0], -1)\n",
    "y = torch.load('y.pt').long()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51eb72be",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "TEST_SIZE = 0.3\n",
    "DROPOUT = 0.5\n",
    "LEARNING_RATE = 3e-4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CLASS_WEIGHT = compute_class_weight('balanced', classes=list(range(7)), y=y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1223955",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_Indiv(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, dropout, lr):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            ts.nn.ParallelLinear(3072, 192),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm1d(192),\n",
    "            nn.Dropout(dropout),\n",
    "            ts.nn.ParallelLinear(192, 192),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed8d83ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, device, train_dataset, val_dataset, batch_size, lr, dropout, class_weight):\n",
    "        super().__init__()\n",
    "        self.comps = [NN_Indiv(dropout, lr).to(device) for _ in range(8)]\n",
    "        self.en_fc = nn.Sequential(\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm1d(768),\n",
    "            nn.Dropout(dropout),\n",
    "            ts.nn.ParallelLinear(768, 384)\n",
    "        )\n",
    "        self.zh_fc = nn.Sequential(\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm1d(768),\n",
    "            nn.Dropout(dropout),\n",
    "            ts.nn.ParallelLinear(768, 384)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm1d(768),\n",
    "            nn.Dropout(dropout),\n",
    "            ts.nn.ParallelLinear(768, 6)\n",
    "        )\n",
    "        # self.criterion = nn.CrossEntropyLoss(weight=class_weight)\n",
    "        self.pos_weights = torch.Tensor([1, .8, .6, .4, .2, 0]).to(device)\n",
    "        self.pos_weights.requires_grad = False\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "    \n",
    "    def forward(self, x):\n",
    "        en, zh = torch.tensor_split(\n",
    "            torch.cat(\n",
    "                [m(t) for m, t in zip(self.comps, torch.tensor_split(x, 8, dim=1))], dim=1),\n",
    "            2, dim=1)\n",
    "        xx = torch.cat((self.en_fc(en), self.zh_fc(zh)), dim=1)\n",
    "        return self.fc(xx)\n",
    "    \n",
    "    def loss_func(self, logits, y):\n",
    "        weighted_pos = (F.softmax(logits, dim=1) * self.pos_weights).sum(axis=1)\n",
    "        y_pos = 1 - y / 5\n",
    "        pos_dis = ((weighted_pos - y_pos) ** 2).sum()\n",
    "        return pos_dis\n",
    "    \n",
    "    def training_step(self, batch, idx):\n",
    "        X, y = batch\n",
    "        logits = self(X)\n",
    "        loss = self.loss_func(logits, y)\n",
    "        self.log(f'train_pos_dis', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, idx):\n",
    "        X, y = batch\n",
    "        logits = self(X)\n",
    "        loss = self.loss_func(logits, y)\n",
    "        self.log(f'val_pos_dis', loss)\n",
    "        return loss\n",
    "    \n",
    "    def sort_notis(self, text):\n",
    "        with torch.no_grad():\n",
    "            X = torch.stack([torch.cat((\n",
    "                torch.cat(en_model(**en_tokenizer(row, return_tensors='pt', padding=True, truncation=True), output_hidden_states=True)[2][-4: ])[:, 0].detach(),\n",
    "                torch.cat(zh_model(**zh_tokenizer(row, return_tensors='pt', padding=True, truncation=True), output_hidden_states=True)[2][-4: ])[:, 0].detach(),\n",
    "                    )) for row in tqdm(text)]).to(self.device)\n",
    "            X = X.view(X.shape[0], -1)\n",
    "            logits = self(X)\n",
    "            _, y_pred = torch.max(logits.data, axis=1)\n",
    "            print(F.softmax(logits, dim=1))\n",
    "            weighted_pos = (F.softmax(logits, dim=1) * self.pos_weights).sum(axis=1).cpu().numpy()\n",
    "            text_score = list(zip(text, weighted_pos))\n",
    "            text_score.sort(key=lambda x: -x[1])\n",
    "        return text_score\n",
    "    \n",
    "    @property\n",
    "    def num_training_steps(self) -> int:\n",
    "        \"\"\"Total training steps inferred from datamodule and devices.\"\"\"\n",
    "        if self.trainer.max_steps:\n",
    "            return self.trainer.max_steps\n",
    "\n",
    "        limit_batches = self.trainer.limit_train_batches\n",
    "        batches = len(self.train_dataloader())\n",
    "        batches = min(batches, limit_batches) if isinstance(limit_batches, int) else int(limit_batches * batches)     \n",
    "\n",
    "        num_devices = max(1, self.trainer.num_gpus, self.trainer.num_processes)\n",
    "        if self.trainer.tpu_cores:\n",
    "            num_devices = max(num_devices, self.trainer.tpu_cores)\n",
    "\n",
    "        effective_accum = self.trainer.accumulate_grad_batches * num_devices\n",
    "        return (batches // effective_accum) * self.trainer.max_epochs\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, self.num_training_steps)\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(train_dataset, batch_size=self.batch_size, num_workers=8, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(val_dataset, batch_size=self.batch_size, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14ce1dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=TEST_SIZE)\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "model = NN(DEVICE, train_dataset, val_dataset, BATCH_SIZE, LEARNING_RATE, DROPOUT, CLASS_WEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c334bb06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "trainer = Trainer(\n",
    "    #auto_lr_find=True,\n",
    "    #auto_scale_batch_size=\"binsearch\",\n",
    "    callbacks=[lr_monitor],\n",
    "    gpus=1,\n",
    "    #logger=False,\n",
    "    max_epochs=1500,\n",
    "    profiler=\"simple\",\n",
    "    stochastic_weight_avg=True,\n",
    "    track_grad_norm=2,\n",
    "    weights_save_path=\"model.pt\",\n",
    ")\n",
    "\n",
    "trainer.tune(model)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "836d1c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model8.pt')\n",
    "for i, m in enumerate(model.comps):\n",
    "    torch.save(m.state_dict(), f'model{i}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44e7c162",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model8.pt'))\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "for i, m in enumerate(model.comps):\n",
    "    model.comps[i].load_state_dict(torch.load(f'model{i}.pt'))\n",
    "    model.comps[i] = model.comps[i].to(DEVICE)\n",
    "    model.comps[i].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65ebc790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 12.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.6560e-04, 7.8779e-01, 1.7674e-03, 1.1068e-01, 2.5234e-05, 9.9370e-02]],\n",
      "       device='cuda:0')\n",
      "[(['Messenger', 'IM', '公司', '急'], 0.67593586)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from random import sample\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "en_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", output_hidden_states=True)\n",
    "en_model = AutoModel.from_pretrained(\"bert-base-cased\", output_hidden_states=True)\n",
    "zh_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\", output_hidden_states=True)\n",
    "zh_model = AutoModel.from_pretrained(\"bert-base-chinese\", output_hidden_states=True)\n",
    "\n",
    "text = [\n",
    "    ['Messenger', 'IM', '公司', '急']\n",
    "]\n",
    "pprint(model.sort_notis(text))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c1124073dee199d8e1894afd18905e6ab65c2b78c2f71f2204c8c819619ccb15"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
